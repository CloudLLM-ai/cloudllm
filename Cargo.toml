[package]
name = "cloudllm"
version = "0.9.0"
authors = ["Angel Leon <gubatron@gmail.com>"]
edition = "2018"
description = "A batteries-included Rust toolkit for building intelligent agents with LLM integration, multi-protocol tool support, and multi-agent orchestration."
repository = "https://github.com/CloudLLM-ai/cloudllm"
license = "MIT"
documentation = "https://docs.rs/cloudllm/latest/cloudllm/"

[dependencies]
tokio = { version = "1.48.0", features = ["full"] }

# This is my own fork of the OpenAI Rust client, which has a few improvements over the original.
# https://github.com/gubatron/openai-rust
openai-rust2 = "1.7.2"

async-trait = "0.1.89"
log = "0.4.28"
env_logger = "0.11.8"
bumpalo = "3.19.0"
lazy_static = "1.5.0"
reqwest = { version = "0.12.24", features = ["json"] }
futures-util = "0.3.31"
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.145"
chrono = { version = "0.4", features = ["serde"] }
evalexpr = "13.0.0"
sha2 = "0.10"
urlencoding = "2.1.3"
axum = { version = "0.8.7", optional = true }
tower = { version = "0.5.2", optional = true }

[dev-dependencies]
tempfile = "3.23"

[features]
# MCP Server builder with HTTP support (requires axum and tower)
mcp-server = ["axum", "tower"]
