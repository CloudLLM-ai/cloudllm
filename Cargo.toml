[package]
name = "cloudllm"
version = "0.6.0"
authors = ["Angel Leon <gubatron@gmail.com>"]
edition = "2018"
description = "A Rust library for bridging applications with remote LLMs across various platforms."
repository = "https://github.com/CloudLLM-ai/cloudllm"
license = "MIT"
documentation = "https://docs.rs/cloudllm/latest/cloudllm/"

[dependencies]
tokio = { version = "1.47.1", features = ["full"] }

# This is my own fork of the OpenAI Rust client, which has a few improvements over the original.
# https://github.com/gubatron/openai-rust
openai-rust2 = { version = "1.6.0" }

async-trait = "0.1.88"
log = "0.4.27"
env_logger = "0.11.8"
bumpalo = "3.16"
lazy_static = "1.5.0"
reqwest = { version = "0.12", features = ["json"] }
futures-util = "0.3"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
chrono = { version = "0.4", features = ["serde"] }
evalexpr = "12.0"
urlencoding = "2.1"
axum = { version = "0.8.6", optional = true }
tower = { version = "0.5.2", optional = true }

[dev-dependencies]
tempfile = "3.10"

[features]
# MCP Server builder with HTTP support (requires axum and tower)
mcp-server = ["axum", "tower"]
